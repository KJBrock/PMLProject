---
title: "Machine Learning"
author: "Kevin Brock"
date: "July 10, 2015"
output: html_document
---
```{r setoptions, echo=FALSE}

library(knitr)
opts_chunk$set(echo=FALSE, results="hide", message=FALSE, warning=FALSE)
               
```

```{r}

library(doParallel)
registerDoParallel(cores=4)

library(randomForest)
library(caret)
library(caretEnsemble)
library(plyr)
library(dplyr)

```

# Overview

We're trying to predict which of five forms of an exercise is being performed based on measurements from sensors worn by the study participants.  One of the categories is a correct performance of the exercise, while each of the others has a specific error in the way the exercise was performed.  The data set consists of 19622 observations of 160 variables.  There are five categories for how the exercise was performed, labeled A, B, C, D and E in the data set.  For our purposes it is irrelevant which of these is correct performance.

You can find more information about the original research [here](http://groupware.les.inf.puc-rio.br/har#wle_paper_section).

The R code which generated these results can be found in the original R Markdown File [here](https://github.com/KJBrock/PMLProject/blob/master/MachineLearningProject.Rmd).

# Preprocessing

I split the data set into a training set, a test set, and a validation set.  The validation data is 30% of the original data, the training data is 49% of the original data, and the test data is 21% of the original data (two 70/30 splits).

The majority of the columns in the data set are essentially empty.  They have more than 19200 empty empty entries out of 19622 total data points. Most of them in fact have exactly 19216.  This indicates that the values which are there aren't related to particular exercises or individuals.  

* I removed the almost entirely empty columns.
* I removed bookkeeping columns which are not relevant to predicting future results, such as X, user_name, timestamps, and windows.
* I converted classe to a factor variable, so that the algorithms would correctly identify this as a classification problem.

The rationale for removing the bookkeeping columns is that all of the information is relevant only to the current data set.  Any future data set from the same devices would have entirely different information in those columns, so using the columns to train on this data set would automatically overfit it to the current data.

```{r}
if( !dir.exists("data") ) {
    dir.create("data")
}

if( !file.exists("data/pml-training.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  "data/pml-training.csv", method="wget")
}

trainingDataRaw <- read.csv('data/pml-training.csv', stringsAsFactors=FALSE)


if( !file.exists("data/pml-testing.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                    "data/pml-testing.csv", method="wget")
}

submissionData <- read.csv('data/pml-testing.csv', stringsAsFactors=FALSE)

set.seed(3141)

classe <- as.factor(trainingDataRaw$classe)
trainingData <- dplyr::select(trainingDataRaw, -classe)

# Some columns are almost entirely NA
includeCol <- sapply(trainingData, function(x) { sum(is.na(x)) } ) == 0
includeNames <- names(includeCol)[includeCol == TRUE]
trainingData <- trainingData[includeNames] 

# Omit bookkeeping

trainingData <- dplyr::select(trainingData, 
        -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2,
            cvtd_timestamp, new_window, num_window))

includeCol <- sapply(trainingData, function(x) { sum(is.na(as.numeric(x))) }) == 0
includeNames <- names(includeCol)[includeCol == TRUE]
trainingData <- trainingData[includeNames] 

# Put back classe
trainingData <- cbind(classe, trainingData)

inBuild <- createDataPartition(trainingData$classe, p=0.7, list=FALSE)

validationData <- trainingData[-inBuild,]
buildData <- trainingData[inBuild,]

inTraining <- createDataPartition(buildData$classe, p=0.7, list=FALSE)
training <- buildData[inTraining,]
testing <- buildData[-inTraining,]

```

## Method Selection

I chose random forests, boosting, and linear discriminant analysis as candidate methods.  The problem is classifying multiple categories, not fitting a line through a numeric continuum or distinguishing two results from each other.  Methods which are primarily for linear phenomena wouldn't be very useful.

For each of the methods we use:

```{r, echo=TRUE, results='markup'}
control <- trainControl(method="cv")
```

I set the seed to 5358 before we run train() each time for the RF, GBM & LDA training.

### Boosting

```{r}
if( !file.exists("modFitGbmFull001.rds")) {
    set.seed(5358)
    modFitGbmFull <- train(classe ~ ., method="gbm", verbose=FALSE, data=training,
                                trControl = control)
    saveRDS(modFitGbmFull, file="modFitGbmFull001.rds")
} else {
    modFitGbmFull <- readRDS("modFitGbmFull001.rds")
}
predGbm <- predict(modFitGbmFull,testing)
table(predGbm, testing$classe)
accGbm <- sum(predGbm == testing$classe)/length(testing$classe)
errGbm <- 1 - accGbm
```

Boosting generated very good results. Using method "gbm" we see an in-sample error rate of `r errGbm`.

### Random Forests

```{r}
if( !file.exists("modFitRfFullCv001.rds")) {
    set.seed(5358)
    modFitRfFullCv <- train(classe ~ ., method="rf", trControl = control,
                            prox=TRUE, data=training)
    saveRDS(modFitRfFullCv, file="modFitRfFullCv001.rds")
} else {
    modFitRfFullCv <-  readRDS("modFitRfFullCv001.rds")
}

predRfFullCv <- predict(modFitRfFullCv,testing)
table(predRfFullCv, testing$classe)
accRfCv <- sum(predRfFullCv == testing$classe)/length(testing$classe)
errRfCv <- 1 - accRfCv
```

Random forests gave me the best results for this problem.  I used method "rf" with prox=TRUE. The in-sample error rate was `r errRfCv`.

### Linear Discriminant Analysis

```{r}
if( !file.exists("modFitLda001.rds")) {
    set.seed(5358)
    modFitLda <- train(classe ~ ., data = training, method = "lda",
                                trControl = control)
    saveRDS(modFitLda, file="modFitLda001.rds")
} else {
    modFitLda <-  readRDS("modFitLda001.rds")
}

predLda <- predict(modFitLda, testing)
table(predLda, testing$classe)
accLda <- sum(predLda == testing$classe)/length(testing$classe)
errLda <- 1-accLda

```

Linear Discriminant Analysis had a much lower accuracy than either random forests or 
boosting.  With method "lda" the in-sample error rate was `r errLda`.

### Stacking

```{r}

stackedDf <- data.frame(predRfFullCv, predGbm, predLda, classe =testing$classe)

if( !file.exists("modFitStackedRf001.rds")) {
    set.seed(2386)
    modFitStackedRf <- train(classe ~ ., method="rf", data = stackedDf, trControl=control)
    saveRDS(modFitStackedRf, file="modFitStackedRf001.rds")
} else {
    modFitStackedRf <-  readRDS("modFitStackedRf001.rds")
}

stackedPred <- predict(modFitStackedRf, stackedDf)
table(stackedPred, testing$classe)
accStackedRf <- sum(stackedPred == testing$classe)/length(testing$classe)
errStacked <- sum(stackedPred != testing$classe)/length(testing$classe)
```

Stacking the lda, rf and gbm results together using rf as the algorithm for training on the stacked data frame gave us only marginally higher accuracy at the cost of higher computational effort.  We got an in-sample error of `r errStacked`. 

### Results

The results for the four methods can be compared in table form.  Running the models through resample() gives us a comparison of the expected accuracy & kappa values for each of them.  Expected error rates are 1 - accuracy.

```{r, results='markup'}
results <- resamples(list(LDA=modFitLda, 
                          GBM=modFitGbmFull, 
                          RF=modFitRfFullCv, 
                          STACKED=modFitStackedRf))
summary(results)
```

We can also see visually that the RF and GBM results are far superior to the LDA results, while the stacked result is extremely close to the random forest's values.

```{r}
bwplot(results)
```

Although the stacked values are better than the random forest values, the fact that stacking requires us to run both random forest and boosting on each data set makes it less desirable in this case for the best method.

Boosting ran significantly faster than random forests on my systems, and so in some cases we might want to use boosting rather than random forests when speed is particularly important.

## Cross Validation and Expected Error Rate

The expected out-of-sample error rate was calculated by testing the models on the validation data set.  The algorithms do cross validation from the training set as part of the training process, and the testing data set is used to train the stacked algorithm, so we kept a separate validation data set from the initial data to find the expected out-of-sample error rate.

```{r}
predRfValid <- predict(modFitRfFullCv, validationData)
table(predRfValid, validationData$classe)
accRfValidation <- sum(predRfValid == validationData$classe)/length(validationData$classe)
errRfValidation <- 1- accRfValidation
predGbmValid <- predict(modFitGbmFull, validationData)
accGbmValidation <- sum(predGbmValid == validationData$classe)/length(validationData$classe)
errGbmValidation <- 1- accGbmValidation
predLdaValid <- predict(modFitLda, validationData)
accLdaValidation <- sum(predLdaValid == validationData$classe)/length(validationData$classe)
errLdaValidation <- 1-accLdaValidation

stackedDfValid <- data.frame(predRfValid, predGbmValid, predLdaValid, classe =validationData$classe)
if( !file.exists("modFitStackedRfValid001.rds")) {
    set.seed(2386)
    modFitStackedRfValid <- train(classe ~ ., method="rf", data = stackedDfValid,
                                trControl = control)
    saveRDS(modFitStackedRfValid, file="modFitStackedRfValid001.rds")
} else {
    modFitStackedRfValid <-  readRDS("modFitStackedRfValid001.rds")
}

predValidStackedRf <- predict(modFitStackedRfValid, validationData)
accStackedValid <- sum(predValidStackedRf == validationData$classe)/length(validationData$classe)
errStackedValid <- 1 - accStackedValid 
```

The out of sample error rates calculated by running the models on the validation data are:

```{r, results='markup'}
as.data.frame(cbind(method=c("RF", "GBM", "LDA"), errorRate = c(errRfValidation, errGbmValidation, errLdaValidation)))
```

Interestingly, when we generated a stacked model from the RF, LDA and GBM results on the validation data set the stacked model gave identical results to the RF model.

## Interpretation

The order of importance for the readings is very similar for the rf and gbm methods.  They agree on the relative importance of the first three, and have most of the top ten variables in common:

```{r, results='markup'}
RfOrder <- order(varImp(modFitRfFullCv)$importance, decreasing = TRUE)
GbmOrder <- order(varImp(modFitGbmFull)$importance, decreasing = TRUE)
data.frame(Gbm=labels(varImp(modFitGbmFull)$importance)[[1]][GbmOrder][1:10], 
           Rf=labels(varImp(modFitRfFullCv)$importance)[[1]][RfOrder][1:10])
```

It seems likely that a domain expert would be able to translate these into specific movement recommendations based on the sensor readings.


```{r, eval=FALSE}

predSubmission <- predict(modFitRfFullCv, submissionData)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

if( !dir.exists("submission") ) {
    dir.create("submission")
}

currentDir <- getwd()
setwd("submission")

pml_write_files(predSubmission)
```