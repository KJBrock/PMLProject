---
title: "Machine Learning"
author: "Kevin Brock"
date: "July 10, 2015"
output: html_document
---
```{r setoptions, echo=FALSE}

library(knitr)
opts_chunk$set(echo=FALSE, results="hide", message=FALSE, warning=FALSE)
               
```

```{r}

library(doParallel)
registerDoParallel(cores=4)

library(randomForest)
library(caret)
library(caretEnsemble)
library(dplyr)

```

# Overview

We're trying to predict which of five forms of an activity is being performed based on measurements from sensors worn by the study participants.  The data set consists of 19622 observations, or 160 variables.

You can find more information about the original research [here](http://groupware.les.inf.puc-rio.br/har#wle_paper_section).

The R code which generated these results can be found in the original R Markdown File [here](https://github.com/KJBrock/PMLProject/blob/master/MachineLearningProject.Rmd).

# Preprocessing

We split the data set into a training set, a test set, and a validation set.  The validation data is 30% of the original data, the training data is 49% of the original data, and the test data is 21% of the original data (two 70/30 splits).

Examining the training data set we see that the majority of the columns are essentially empty.  They have more than 19200 empty empty entries out of 19622 total data points. Most of them in fact have exactly 19216.  This indicates that the values which are there aren't related to particular exercises or individuals.  

We removed those columns, as well as bookkeeping columns which are not relevant to predicting future results, such as X, user_name, timestamps, and windows.

We also converted classe to a factor variable, so that the algorithms would correctly identify this as a classification problem.

```{r, cache=TRUE}
if( !dir.exists("data") ) {
    dir.create("data")
}

if( !file.exists("data/pml-training.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                  "data/pml-training.csv", method="wget")
}

trainingDataRaw <- read.csv('data/pml-training.csv', stringsAsFactors=FALSE)


if( !file.exists("data/pml-testing.csv")) {
    download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                    "data/pml-testing.csv", method="wget")
}

submissionData <- read.csv('data/pml-testing.csv', stringsAsFactors=FALSE)

set.seed(3141)

classe <- as.factor(trainingDataRaw$classe)
trainingData <- dplyr::select(trainingDataRaw, -classe)

# Some columns are almost entirely NA
includeCol <- sapply(trainingData, function(x) { sum(is.na(x)) } ) == 0
includeNames <- names(includeCol)[includeCol == TRUE]
trainingData <- trainingData[includeNames] 

# Omit bookkeeping

trainingData <- dplyr::select(trainingData, 
        -c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2,
            cvtd_timestamp, new_window, num_window))

includeCol <- sapply(trainingData, function(x) { sum(is.na(as.numeric(x))) }) == 0
includeNames <- names(includeCol)[includeCol == TRUE]
trainingData <- trainingData[includeNames] 

# Put back classe
trainingData <- cbind(classe, trainingData)

inBuild <- createDataPartition(trainingData$classe, p=0.7, list=FALSE)

validationData <- trainingData[-inBuild,]
buildData <- trainingData[inBuild,]

inTraining <- createDataPartition(buildData$classe, p=0.7, list=FALSE)
training <- buildData[inTraining,]
testing <- buildData[-inTraining,]

```

## Method Selection

I chose random forests, boosting, and linear discriminant analysis as candidate methods.  The problem is classifying multiple categories, not fitting a line through a numeric continuum or distinguishing two results from each other, 

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=3)
```

### Boosting

```{r, cache=TRUE}
if( !file.exists("modFitGbmFull001.rds")) {
    set.seed(5358)
    modFitGbmFull <- train(classe ~ ., method="gbm", verbose=FALSE, data=training)
    saveRDS(modFitGbmFull, file="modFitGbmFull001.rds")
} else {
    modFitGbmFull <- readRDS("modFitGbmFull001.rds")
}
predGbm <- predict(modFitGbmFull,testing)
table(predGbm, testing$classe)
accGbm <- sum(predGbm == testing$classe)/length(testing$classe)
errGbm <- 1 - accGbm
```

Boosting generated very good results, using train() with method "gbm".  We got an in-sample error rate of `r errGbm`.

### Random Forests

```{r, cache=TRUE}
if( !file.exists("modFitRfFullCv001.rds")) {
    set.seed(5358)
    modFitRfFullCv <- train(classe ~ ., method="rf", trControl = control,
                            prox=TRUE, data=training)
    saveRDS(modFitRfFullCv, file="modFitRfFullCv001.rds")
} else {
    modFitRfFullCv <-  readRDS("modFitRfFullCv001.rds")
}

predRfFullCv <- predict(modFitRfFullCv,testing)
table(predRfFullCv, testing$classe)
accRfCv <- sum(predRfFullCv == testing$classe)/length(testing$classe)
errRfCv <- 1 - accRfCv
```

Random forests gave me the best results for this problem. The in-sample error rate was `r errRfCv`.

### Linear Discriminant Analysis

```{r, cache=TRUE}
if( !file.exists("modFitLda001.rds")) {
    set.seed(5358)
    modFitLda <- train(classe ~ ., data = training, method = "lda")
    saveRDS(modFitLda, file="modFitLda001.rds")
} else {
    modFitLda <-  readRDS("modFitLda001.rds")
}

predLda <- predict(modFitLda, testing)
table(predLda, testing$classe)
accLda <- sum(predLda == testing$classe)/length(testing$classe)
errLda <- 1-accLda

```

Linear Discriminant Analysis had a much lower accuracy than either random forests or 
boosting.  The in-sample error rate was `r errLda`.

### Stacking

```{r, cache=TRUE}

stackedDf <- data.frame(predRf, predGbm, predLda, classe =testing$classe)

if( !file.exists("modFitStackedRf001.rds")) {
    set.seed(2386)
    modFitStackedRf <- train(classe ~ ., method="rf", data = stackedDf)
    saveRDS(modFitStackedRf, file="modFitStackedRf001.rds")
} else {
    modFitStackedRf <-  readRDS("modFitStackedRf001.rds")
}

stackedPred <- predict(modFitStackedRf, stackedDf)
table(stackedPred, testing$classe)
accStackedRf <- sum(stackedPred == testing$classe)/length(testing$classe)
errStacked <- sum(stackedPred != testing$classe)/length(testing$classe)
```

Stacking the lda, rf and gbm results together using rf as the algorithm for training on the stacked data frame gave us only marginally higher accuracy at the cost of much higher computational effort.  We got an in-sample error of `r errStacked`. 

### Results

The results for the four methods can be compared in table form.  Running the models through resample() gives us a comparison of the expected accuracy & kappa values for each of them.  Expected error rates are 1 - accuracy.

```{r, results='markup'}
results <- resamples(list(LDA=modFitLda, 
                          GBM=modFitGbmFull, 
                          RF=modFitRfFull, 
                          STACKED=modFitStackedRf))
summary(results)
```

We can also see visually that the RF and GBM results are far superior, while the stacked result is extremely close to the random forest's values.

```{r}
bwplot(results)
```

Although the stacked values are better than the random forest values, the fact that stacking requires us to run both random forest and boosting on each data set makes it less desirable in this case for the best method.

Boosting ran significantly faster than random forests on my systems, and so in some cases we might want to use boosting rather than random forests when speed is particularly important.

## Cross Validation and Expected Error Rate

The expected out-of-sample error rate was calculated by testing the models on the validation data set.  The algorithms do cross validation from the testing set as part of the training process, and the testing data set is used to train the stacked algorithm, so we kept a separate validation data set from the initial data to find the expected out-of-sample error rate.

```{r, cache=TRUE}
predRfValid <- predict(modFitRfFullCv, validationData)
table(predRfValid, validationData$classe)
accRfValidation <- sum(predRfValid == validationData$classe)/length(validationData$classe)
errRfValidation <- 1- accRfValidation
predGbmValid <- predict(modFitGbmFull, validationData)
accGbmValidation <- sum(predGbmValid == validationData$classe)/length(validationData$classe)
errGbmValidation <- 1- accGbmValidation
predLdaValid <- predict(modFitLda, validationData)
accLdaValidation <- sum(predLdaValid == validationData$classe)/length(validationData$classe)
errLdaValidation <- 1-accLdaValidation

stackedDfValid <- data.frame(predRfValid, predGbmValid, predLdaValid, classe =validationData$classe)
if( !file.exists("modFitStackedRfValid001.rds")) {
    set.seed(2386)
    modFitStackedRfValid <- train(classe ~ ., method="rf", data = stackedDfValid)
    saveRDS(modFitStackedRfValid, file="modFitStackedRfValid001.rds")
} else {
    modFitStackedRfValid <-  readRDS("modFitStackedRfValid001.rds")
}

predValidStackedRf <- predict(modFitStackedRfValid, validationData)
accStackedValid <- sum(predValidStackedRf == validationData$classe)/length(validationData$classe)
errStackedValid <- 1 - accStackedValid 
table
```

```{r, results='markup'}
as.data.frame(cbind(method=c("RF", "GBM", "LDA", "STACKED"), errorRate = c(errRfValidation, errGbmValidation, errLdaValidation, errStackedValid)))
```

```{r, eval=FALSE, cache=TRUE}

predSubmission <- predict(modFitRfFullCv, submissionData)

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

if( !dir.exists("submission") ) {
    dir.create("submission")
}

currentDir <- getwd()
setwd("submission")

pml_write_files(predSubmission)
```